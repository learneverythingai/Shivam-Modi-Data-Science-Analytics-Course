{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learneverythingai/Shivam-Modi-Data-Science-Analytics-Course/blob/main/Advanced%20Deep%20Learning%20Course/bert_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Author and Instructor of this Notebook is **Shivam Modi**.\n",
        "## LinkedIn: https://www.linkedin.com/in/shivam-modi-datascientist/"
      ],
      "metadata": {
        "id": "MyDtBZ_m99uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as scheduler\n",
        "import transformers\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from tqdm.autonotebook import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "trusted": true,
        "id": "d6dgGoLzDwEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    device='cuda'if torch.cuda.is_available() else 'cpu'\n",
        "    train_batch_size=8\n",
        "    val_batch_size=8\n",
        "    seed=5\n",
        "    n_splits=5\n",
        "    pooler_output_dim=768\n",
        "    metric=f1_score\n",
        "    lr=1e-4\n",
        "    step_size=10\n",
        "    gamma=0.1\n",
        "    epochs=5"
      ],
      "metadata": {
        "trusted": true,
        "id": "wpXxdy6BDwE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint=\"bert-base-cased\"\n",
        "bert=transformers.AutoModel.from_pretrained(checkpoint)\n",
        "bert_tokenizer=transformers.AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8lA_qYpyDwE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n",
        "data['sentiment']=list(map(lambda x : 1 if x==\"positive\" else 0,data['sentiment'].values))\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "data.loc[:,'kfold']=-1\n",
        "data.sample(frac=1,random_state=Config.seed).reset_index(drop=True)\n",
        "labels=data['sentiment'].values\n",
        "skf=StratifiedKFold(n_splits=Config.n_splits)\n",
        "\n",
        "for fold,(train,val) in enumerate(skf.split(X=data,y=data['sentiment'])):\n",
        "    data.loc[val,'kfold']=fold\n",
        "data.to_csv('/kaggle/working/folds.csv',index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wnJ-Oe9oDwE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewData:\n",
        "    def __init__(self,data):\n",
        "        self.data=data.reset_index(drop=True)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        review=bert_tokenizer(self.data['review'][idx],truncation=True,max_length=300,padding='max_length',return_tensors='pt')\n",
        "        sentiment=torch.tensor(self.data['sentiment'][idx],dtype=torch.long)\n",
        "        return {'x':{k:v.to(device=Config.device) for k,v in review.items()},'y':sentiment.to(device=Config.device)}\n",
        "\n",
        "    \n",
        "    \n",
        "class Classif_Model(nn.Module):\n",
        "    def __init__(self,bert_model,hidden_shape,out_shape):\n",
        "        super().__init__()\n",
        "        self.bert_model=bert_model\n",
        "        self.hidden_shape=hidden_shape\n",
        "        self.out_shape=out_shape\n",
        "        self.layers=nn.Sequential(\n",
        "            nn.Linear(Config.pooler_output_dim,hidden_shape),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(hidden_shape,out_shape)\n",
        "        \n",
        "        )\n",
        "    \n",
        "    def forward(self,batch):\n",
        "        out=self.bert_model(batch)\n",
        "        out=out['pooler_output']\n",
        "        out=self.layers(out)\n",
        "        return out\n",
        "    \n",
        "model=Classif_Model(bert,100,2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0Cg-S_a6DwE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Train:\n",
        "    def __init__(self,model,criterion,optimizer,scheduler):\n",
        "        self.model=model\n",
        "        self.criterion=criterion\n",
        "        self.optimizer=optimizer\n",
        "        self.scheduler=scheduler\n",
        "    \n",
        "    @staticmethod\n",
        "    def logits_to_prediction(output):\n",
        "        preds=torch.argmax(torch.softmax(output.detach().to('cpu'),dim=-1),dim=-1)\n",
        "        return preds.view(1,-1)\n",
        "    @staticmethod\n",
        "    def return_metric(preds,labels):\n",
        "        return Config.metric(preds.to('cpu'),labels.to('cpu'),average='weighted')\n",
        "    \n",
        "    def fit(self,data_loader):\n",
        "        self.model.train()\n",
        "        for batch in data_loader:\n",
        "            out=self.model(batch['x']['input_ids'].squeeze_(1)) \n",
        "            loss=self.criterion(out,batch['y'])\n",
        "            loss.backward()\n",
        "            self.optimizer.zero_grad()\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "            \n",
        "    def predict_metric(self,data_loader,type_='train'):\n",
        "        batch_loss=0\n",
        "        if type_=='train':\n",
        "            batch_size=Config.train_batch_size\n",
        "        elif type_=='val':\n",
        "            batch_size=Config.val_batch_size\n",
        "            \n",
        "        preds=torch.empty([len(data_loader),batch_size])\n",
        "        labels=torch.empty([len(data_loader),batch_size])\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            for idx,batch in enumerate(data_loader):\n",
        "                out=self.model(batch['x']['input_ids'].squeeze_(1)) \n",
        "                batch_loss+=self.criterion(out,batch['y']).item()\n",
        "                preds[idx,:]=self.logits_to_prediction(out)\n",
        "                labels[idx,:]=batch['y'].to('cpu')\n",
        "        \n",
        "            metric=self.return_metric(preds,labels)\n",
        "        return metric,batch_loss/len(data_loader)\n",
        "    \n",
        "             \n",
        "            \n",
        "folds=pd.read_csv(\"/kaggle/working/folds.csv\")\n",
        "model=model.to(device=Config.device)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=Config.lr)\n",
        "scheduler=optim.lr_scheduler.StepLR(optimizer,step_size=Config.step_size,gamma=Config.gamma)\n",
        "train=Train(model,criterion,optimizer,scheduler)\n",
        "\n",
        "epoch_train_loss=[] #per epoch\n",
        "epoch_val_loss=[]   #per epoch\n",
        "epoch_train_f1=[]   #per epoch\n",
        "epoch_val_f1=[]     #per epoch\n",
        "\n",
        "for epoch in tqdm(range(Config.epochs)):\n",
        "\n",
        "    train_loss=[] \n",
        "    val_loss=[]   \n",
        "    train_f1=[]   \n",
        "    val_f1=[]  \n",
        "    \n",
        "    for fold in range(Config.n_splits):\n",
        "        \n",
        "        train_data=DataLoader(ReviewData(folds.loc[folds['kfold']!=fold]),batch_size=Config.train_batch_size,drop_last=True)\n",
        "        val_data=DataLoader(ReviewData(folds.loc[folds['kfold']==fold]),batch_size=Config.val_batch_size,drop_last=True)\n",
        "        train.fit(train_data)\n",
        "        \n",
        "        t_f1,t_loss=train.predict_metric(train_data,'train')\n",
        "        train_loss.append(t_loss)\n",
        "        train_f1.append(t_f1)\n",
        "        \n",
        "        \n",
        "        v_f1,v_loss=train.predict_metric(val_data,'val')\n",
        "        val_loss.append(v_loss)\n",
        "        val_f1.append(v_f1)\n",
        "        \n",
        "    epoch_train_loss.append(np.mean(train_loss))\n",
        "    epoch_train_f1.append(np.mean(train_f1))\n",
        "    epoch_val_loss.append(np.mean(val_loss))\n",
        "    epoch_val_f1.append(np.mean(val_f1))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "U8iR7E5zDwE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(Config.epochs),epoch_train_loss)\n",
        "plt.plot(range(Config.epochs),epoch_val_loss)\n",
        "plt.plot(range(Config.epochs),epoch_train_f1)\n",
        "plt.plot(range(Config.epochs),epoch_val_f1)\n",
        "plt.legend(['train_loss','val_loss','train_f1','val_f1'],bbox_to_anchor=(1.05,1),loc='upper left');"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZQ3eDo91DwE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mdS9YwuDwE-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}